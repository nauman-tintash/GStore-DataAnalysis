{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tintash/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (3,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903653, 56)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channelGrouping</th>\n",
       "      <th>date</th>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>socialEngagementType</th>\n",
       "      <th>visitId</th>\n",
       "      <th>visitNumber</th>\n",
       "      <th>visitStartTime</th>\n",
       "      <th>device.browser</th>\n",
       "      <th>...</th>\n",
       "      <th>trafficSource.adwordsClickInfo.isVideoAd</th>\n",
       "      <th>trafficSource.adwordsClickInfo.page</th>\n",
       "      <th>trafficSource.adwordsClickInfo.slot</th>\n",
       "      <th>trafficSource.campaign</th>\n",
       "      <th>trafficSource.campaignCode</th>\n",
       "      <th>trafficSource.isTrueDirect</th>\n",
       "      <th>trafficSource.keyword</th>\n",
       "      <th>trafficSource.medium</th>\n",
       "      <th>trafficSource.referralPath</th>\n",
       "      <th>trafficSource.source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>1131660440785968503</td>\n",
       "      <td>1131660440785968503_1472830385</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472830385</td>\n",
       "      <td>1</td>\n",
       "      <td>1472830385</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not provided)</td>\n",
       "      <td>organic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>377306020877927890</td>\n",
       "      <td>377306020877927890_1472880147</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472880147</td>\n",
       "      <td>1</td>\n",
       "      <td>1472880147</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not provided)</td>\n",
       "      <td>organic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>3895546263509774583</td>\n",
       "      <td>3895546263509774583_1472865386</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472865386</td>\n",
       "      <td>1</td>\n",
       "      <td>1472865386</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not provided)</td>\n",
       "      <td>organic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>4763447161404445595</td>\n",
       "      <td>4763447161404445595_1472881213</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472881213</td>\n",
       "      <td>1</td>\n",
       "      <td>1472881213</td>\n",
       "      <td>UC Browser</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google + online</td>\n",
       "      <td>organic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Organic Search</td>\n",
       "      <td>20160902</td>\n",
       "      <td>27294437909732085</td>\n",
       "      <td>27294437909732085_1472822600</td>\n",
       "      <td>Not Socially Engaged</td>\n",
       "      <td>1472822600</td>\n",
       "      <td>2</td>\n",
       "      <td>1472822600</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>(not provided)</td>\n",
       "      <td>organic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 channelGrouping      date        fullVisitorId  \\\n",
       "0           0  Organic Search  20160902  1131660440785968503   \n",
       "1           1  Organic Search  20160902   377306020877927890   \n",
       "2           2  Organic Search  20160902  3895546263509774583   \n",
       "3           3  Organic Search  20160902  4763447161404445595   \n",
       "4           4  Organic Search  20160902    27294437909732085   \n",
       "\n",
       "                        sessionId  socialEngagementType     visitId  \\\n",
       "0  1131660440785968503_1472830385  Not Socially Engaged  1472830385   \n",
       "1   377306020877927890_1472880147  Not Socially Engaged  1472880147   \n",
       "2  3895546263509774583_1472865386  Not Socially Engaged  1472865386   \n",
       "3  4763447161404445595_1472881213  Not Socially Engaged  1472881213   \n",
       "4    27294437909732085_1472822600  Not Socially Engaged  1472822600   \n",
       "\n",
       "   visitNumber  visitStartTime device.browser         ...           \\\n",
       "0            1      1472830385         Chrome         ...            \n",
       "1            1      1472880147        Firefox         ...            \n",
       "2            1      1472865386         Chrome         ...            \n",
       "3            1      1472881213     UC Browser         ...            \n",
       "4            2      1472822600         Chrome         ...            \n",
       "\n",
       "  trafficSource.adwordsClickInfo.isVideoAd  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "\n",
       "  trafficSource.adwordsClickInfo.page trafficSource.adwordsClickInfo.slot  \\\n",
       "0                                 NaN                                 NaN   \n",
       "1                                 NaN                                 NaN   \n",
       "2                                 NaN                                 NaN   \n",
       "3                                 NaN                                 NaN   \n",
       "4                                 NaN                                 NaN   \n",
       "\n",
       "  trafficSource.campaign  trafficSource.campaignCode  \\\n",
       "0              (not set)                         NaN   \n",
       "1              (not set)                         NaN   \n",
       "2              (not set)                         NaN   \n",
       "3              (not set)                         NaN   \n",
       "4              (not set)                         NaN   \n",
       "\n",
       "  trafficSource.isTrueDirect trafficSource.keyword trafficSource.medium  \\\n",
       "0                        NaN        (not provided)              organic   \n",
       "1                        NaN        (not provided)              organic   \n",
       "2                        NaN        (not provided)              organic   \n",
       "3                        NaN       google + online              organic   \n",
       "4                       True        (not provided)              organic   \n",
       "\n",
       "  trafficSource.referralPath trafficSource.source  \n",
       "0                        NaN               google  \n",
       "1                        NaN               google  \n",
       "2                        NaN               google  \n",
       "3                        NaN               google  \n",
       "4                        NaN               google  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_modified.csv') # Important!!\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'channelGrouping', 'date', 'fullVisitorId',\n",
       "       'sessionId', 'socialEngagementType', 'visitId', 'visitNumber',\n",
       "       'visitStartTime', 'device.browser', 'device.browserSize',\n",
       "       'device.browserVersion', 'device.deviceCategory',\n",
       "       'device.flashVersion', 'device.isMobile', 'device.language',\n",
       "       'device.mobileDeviceBranding', 'device.mobileDeviceInfo',\n",
       "       'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',\n",
       "       'device.mobileInputSelector', 'device.operatingSystem',\n",
       "       'device.operatingSystemVersion', 'device.screenColors',\n",
       "       'device.screenResolution', 'geoNetwork.city', 'geoNetwork.cityId',\n",
       "       'geoNetwork.continent', 'geoNetwork.country',\n",
       "       'geoNetwork.latitude', 'geoNetwork.longitude', 'geoNetwork.metro',\n",
       "       'geoNetwork.networkDomain', 'geoNetwork.networkLocation',\n",
       "       'geoNetwork.region', 'geoNetwork.subContinent', 'totals.bounces',\n",
       "       'totals.hits', 'totals.newVisits', 'totals.pageviews',\n",
       "       'totals.transactionRevenue', 'totals.visits',\n",
       "       'trafficSource.adContent',\n",
       "       'trafficSource.adwordsClickInfo.adNetworkType',\n",
       "       'trafficSource.adwordsClickInfo.criteriaParameters',\n",
       "       'trafficSource.adwordsClickInfo.gclId',\n",
       "       'trafficSource.adwordsClickInfo.isVideoAd',\n",
       "       'trafficSource.adwordsClickInfo.page',\n",
       "       'trafficSource.adwordsClickInfo.slot', 'trafficSource.campaign',\n",
       "       'trafficSource.campaignCode', 'trafficSource.isTrueDirect',\n",
       "       'trafficSource.keyword', 'trafficSource.medium',\n",
       "       'trafficSource.referralPath', 'trafficSource.source'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data.sample(frac=0.8,random_state=200)\n",
    "test_df = train_data.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantAttributes = ['visitNumber', 'date', 'fullVisitorId', 'channelGrouping',\n",
    "#                       'device.deviceCategory',\n",
    "                      'device.operatingSystem', 'device.browser',\n",
    "                       'geoNetwork.continent', 'geoNetwork.country',\n",
    "                      'totals.hits', 'totals.pageviews', \n",
    "                      'totals.transactionRevenue']\n",
    "#                        'fullVisitorId']\n",
    "#                       'year', 'month', 'weekday',\n",
    "#                       'mean_hits_per_day', 'mean_pageviews_per_day', 'sum_hits_per_day', 'sum_pageviews_per_day']\n",
    "\n",
    "train_data = train_data[relevantAttributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_df['totals.transactionRevenue']\n",
    "train_X = train_df.drop(labels=['totals.transactionRevenue'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainTrue_class = train_Y.copy()\n",
    "y_trainTrue_class[y_trainTrue_class>0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-65edfa9cad28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mregr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mregr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'google'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr_model = DecisionTreeRegressor(max_depth=2)\n",
    "regr_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tintash/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2901: DtypeWarning: Columns (3,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/home/tintash/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:191: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "/home/tintash/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:192: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds.\n",
      "[250]\ttraining's rmse: 1.478\tvalid_1's rmse: 1.66632\n",
      "[500]\ttraining's rmse: 1.40954\tvalid_1's rmse: 1.66707\n",
      "Early stopping, best iteration is:\n",
      "[384]\ttraining's rmse: 1.43964\tvalid_1's rmse: 1.66569\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features_ is 85 and input n_features is 86 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-01869412bd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-01869412bd24>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sum_of_logs, nrows)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_used_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m#Submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-01869412bd24>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(train_df, test_df, not_used_cols)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     verbose=VERBOSE_EVAL, early_stopping_rounds=STOP_ROUNDS)\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_used_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m                              \u001b[0;34m\"match the input. Model n_features_ is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                              % (self._n_features, n_features))\n\u001b[0m\u001b[1;32m    563\u001b[0m         return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,\n\u001b[1;32m    564\u001b[0m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 85 and input n_features is 86 "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "  \n",
    "NUM_ROUNDS = 20000\n",
    "VERBOSE_EVAL = 250\n",
    "STOP_ROUNDS = 250\n",
    "N_SPLITS = 9\n",
    "TIME_SPLIT = True\n",
    "\n",
    "\n",
    "def process_date_time(data_df):\n",
    "#     logger.info('Start date')\n",
    "    data_df['date'] = data_df['date'].astype(str)\n",
    "    data_df[\"date\"] = data_df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n",
    "    data_df[\"date\"] = pd.to_datetime(data_df[\"date\"])   \n",
    "    data_df[\"year\"] = data_df['date'].dt.year\n",
    "    data_df[\"month\"] = data_df['date'].dt.month\n",
    "    data_df[\"day\"] = data_df['date'].dt.day\n",
    "    data_df[\"weekday\"] = data_df['date'].dt.weekday\n",
    "    data_df['weekofyear'] = data_df['date'].dt.weekofyear\n",
    "    data_df['month_unique_user_count'] = data_df.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "    data_df['day_unique_user_count'] = data_df.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "    data_df['weekday_unique_user_count'] = data_df.groupby('weekday')['fullVisitorId'].transform('nunique')\n",
    "    data_df['weekofyear_unique_user_count'] = data_df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n",
    "#     logger.info('Done with date')\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def process_format(data_df):\n",
    "#     logger.info('Start format')\n",
    "    for col in ['visitNumber', 'totals.hits', 'totals.pageviews']:\n",
    "        data_df[col] = data_df[col].astype(float)\n",
    "    data_df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n",
    "    data_df['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n",
    "#     logger.info('Done with format')\n",
    "    return data_df\n",
    "    \n",
    "def process_device(data_df):\n",
    "#     logger.info('Start device')\n",
    "    data_df['browser_category'] = data_df['device.browser'] + '_' + data_df['device.deviceCategory']\n",
    "    data_df['browser_os'] = data_df['device.browser'] + '_' + data_df['device.operatingSystem']\n",
    "    data_df['category_os'] = data_df['device.deviceCategory'] + '_' + data_df['device.operatingSystem']\n",
    "    data_df['visits_id_browser_mean'] = np.log1p(data_df.groupby(['device.browser'])['visitNumber'].transform('mean'))\n",
    "    data_df['visits_id_os_mean'] = np.log1p(data_df.groupby(['device.operatingSystem'])['visitNumber'].transform('mean'))\n",
    "    data_df['visits_id_cat_mean'] = np.log1p(data_df.groupby(['device.deviceCategory'])['visitNumber'].transform('mean'))\n",
    "    data_df['browser_unique_user_count'] = data_df.groupby('device.browser')['fullVisitorId'].transform('nunique')\n",
    "    data_df['os_unique_user_count'] = data_df.groupby('device.operatingSystem')['fullVisitorId'].transform('nunique')\n",
    "#     logger.info('Done with device')\n",
    "    return data_df\n",
    "\n",
    "def process_totals(data_df):\n",
    "#     logger.info('Start totals')\n",
    "    #data_df['visitNumber'] = data_df['visitNumber']\n",
    "    data_df['visits_id_sum'] = data_df.groupby(['fullVisitorId'])['visitNumber'].transform('sum')\n",
    "    data_df['visits_id_min'] =  data_df.groupby(['fullVisitorId'])['visitNumber'].transform('min')\n",
    "    data_df['visits_id_max'] = data_df.groupby(['fullVisitorId'])['visitNumber'].transform('max')\n",
    "    data_df['visits_id_mean'] = data_df.groupby(['fullVisitorId'])['visitNumber'].transform('mean')\n",
    "    data_df['visits_id_nunique'] = data_df.groupby('visitNumber')['fullVisitorId'].transform('nunique')\n",
    "    #data_df['totals_hits'] = data_df['totals_hits']\n",
    "    data_df['hits_id_sum'] = data_df.groupby(['fullVisitorId'])['totals.hits'].transform('sum')\n",
    "    data_df['hits_id_cnt'] = data_df.groupby(['fullVisitorId'])['totals.hits'].transform('count')\n",
    "    data_df['totals_pageviews'] = data_df['totals.pageviews'].fillna(0)\n",
    "    data_df['pageviews_id_sum'] = data_df.groupby(['fullVisitorId'])['totals.pageviews'].transform('sum')\n",
    "    data_df['pageviews_id_cnt'] = data_df.groupby(['fullVisitorId'])['totals.pageviews'].transform('count')\n",
    "    data_df['mean_hits_per_day'] = data_df.groupby(['day'])['totals.hits'].transform('mean')\n",
    "    data_df['sum_hits_per_day'] = data_df.groupby(['day'])['totals.hits'].transform('sum')\n",
    "    data_df['max_hits_per_day'] = data_df.groupby(['day'])['totals.hits'].transform('max')\n",
    "    data_df['min_hits_per_day'] = data_df.groupby(['day'])['totals.hits'].transform('min')\n",
    "    data_df['var_hits_per_day'] = data_df.groupby(['day'])['totals.hits'].transform('var')\n",
    "    data_df['mean_pageviews_per_day'] = data_df.groupby(['day'])['totals.pageviews'].transform('mean')\n",
    "    data_df['sum_pageviews_per_day'] = data_df.groupby(['day'])['totals.pageviews'].transform('sum')\n",
    "    data_df['max_pageviews_per_day'] = data_df.groupby(['day'])['totals.pageviews'].transform('max')\n",
    "    data_df['min_pageviews_per_day'] = data_df.groupby(['day'])['totals.pageviews'].transform('min')   \n",
    "    \n",
    "#     logger.info('Done with totals')\n",
    "    return data_df\n",
    "\n",
    "def process_geo_network(data_df):\n",
    "#     logger.info('Start geoNetworks')\n",
    "    data_df['mean_visit_per_network_domain']= data_df.groupby('geoNetwork.networkDomain')['visitNumber'].transform('mean')\n",
    "    data_df['min_visit_per_network_domain']= data_df.groupby('geoNetwork.networkDomain')['visitNumber'].transform('min')\n",
    "    data_df['max_visit_per_network_domain']= data_df.groupby('geoNetwork.networkDomain')['visitNumber'].transform('max')\n",
    "    data_df['sum_pageviews_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals_pageviews'].transform('sum')\n",
    "    data_df['count_pageviews_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n",
    "    data_df['mean_pageviews_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n",
    "    data_df['max_pageviews_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('max')\n",
    "    data_df['sum_hits_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n",
    "    data_df['count_hits_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n",
    "    data_df['mean_hits_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n",
    "    data_df['max_hits_per_network_domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('max')\n",
    "    data_df['network_domain_id_nunique'] = data_df.groupby('geoNetwork.networkDomain')['fullVisitorId'].transform('nunique')\n",
    "#     logger.info('Done with geoNetworks')\n",
    "    return data_df\n",
    "\n",
    "def process_traffic_source(data_df):\n",
    "#     logger.info('Start trafficSource')\n",
    "    data_df['source_country'] = data_df['trafficSource.source'] + '_' + data_df['geoNetwork.country']\n",
    "    data_df['campaign_medium'] = data_df['trafficSource.campaign'] + '_' + data_df['trafficSource.medium']\n",
    "    data_df['campaign_country'] = data_df['trafficSource.campaign'] + '_' + data_df['geoNetwork.country']\n",
    "    data_df['medium_hits_mean'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('mean')\n",
    "    data_df['medium_hits_max'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('max')\n",
    "    data_df['medium_hits_min'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('min')\n",
    "    data_df['medium_hits_sum'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('sum')\n",
    "    data_df['medium_hits_var'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('var')\n",
    "#     logger.info('Done with trafficSource')\n",
    "    return data_df\n",
    "\n",
    "def drop_convert_columns(train_df=None, test_df=None):\n",
    "#     logger.info('Start drop convert')\n",
    "    cols_to_drop = [col for col in train_df.columns if train_df[col].nunique(dropna=False) == 1]\n",
    "    train_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    test_df.drop([col for col in cols_to_drop if col in test_df.columns], axis=1, inplace=True)\n",
    "    ###only one not null value\n",
    "    train_df.drop(['trafficSource.campaignCode'], axis=1, inplace=True)\n",
    "    ###converting columns format\n",
    "    train_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].astype(float)\n",
    "    train_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].fillna(0)\n",
    "    train_df['totals.transactionRevenue'] = np.log1p(train_df['totals.transactionRevenue'])\n",
    "#     logger.info('Done with  drop convert')\n",
    "    return train_df, test_df\n",
    "    \n",
    "    \n",
    "def process_categorical_columns(train_df=None, test_df=None):\n",
    "    ## Categorical columns\n",
    "#     logger.info('Process categorical columns ...')\n",
    "    num_cols = ['month_unique_user_count', 'day_unique_user_count', 'weekday_unique_user_count', 'weekofyear_unique_user_count',\n",
    "                'visits_id_browser_mean', 'visits_id_os_mean','visits_id_cat_mean',\n",
    "                'visitNumber',  'visits_id_sum', 'visits_id_min', 'visits_id_max', 'visits_id_mean', 'visits_id_nunique',\n",
    "                'browser_unique_user_count', 'os_unique_user_count',\n",
    "                'totals_hits', 'hits_id_sum', 'hits_id_cnt',\n",
    "                'totals_pageviews', 'pageviews_id_sum', 'pageviews_id_cnt',\n",
    "                'mean_visit_per_network_domain','min_visit_per_network_domain','max_visit_per_network_domain',\n",
    "                'mean_hits_per_day', 'sum_hits_per_day', 'min_hits_per_day', 'max_hits_per_day', 'var_hits_per_day',\n",
    "                'mean_pageviews_per_day', 'sum_pageviews_per_day', 'min_pageviews_per_day', 'max_pageviews_per_day',\n",
    "                'sum_pageviews_per_network_domain', 'count_pageviews_per_network_domain', 'mean_pageviews_per_network_domain','max_pageviews_per_network_domain',\n",
    "                'sum_hits_per_network_domain', 'count_hits_per_network_domain', 'mean_hits_per_network_domain', 'max_hits_per_network_domain', 'network_domain_id_nunique',\n",
    "                'medium_hits_mean','medium_hits_min','medium_hits_max','medium_hits_sum', 'medium_hits_var']\n",
    "    \n",
    "    not_used_cols = [\"date\", \"fullVisitorId\", \"sessionId\", \n",
    "            \"visitId\", \"visitStartTime\", 'totals.transactionRevenue', 'trafficSource.referralPath']\n",
    "    cat_cols = [col for col in train_df.columns if col not in num_cols and col not in not_used_cols]\n",
    "    for col in cat_cols:\n",
    "#         logger.info('Process categorical columns:{}'.format(col))\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n",
    "        train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n",
    "        test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n",
    "    return not_used_cols, train_df, test_df\n",
    "\n",
    "def model(train_df=None, test_df=None,not_used_cols=None):\n",
    "#     logger.info('Start prepare model')\n",
    "    train_df = train_df.sort_values('date')\n",
    "    X_test = test_df.drop([col for col in not_used_cols if col in test_df.columns], axis=1)\n",
    "    \n",
    "    ## Model parameters\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"num_leaves\" : 30,\n",
    "        \"min_child_samples\" : 100,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 20180926,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "   \n",
    "    ## Model\n",
    "#     logger.info('Start traininig model')\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators = NUM_ROUNDS, nthread = 4, n_jobs = -1)\n",
    "    \n",
    "    prediction = np.zeros(test_df.shape[0])\n",
    "    \n",
    "    if(TIME_SPLIT):\n",
    "        import datetime\n",
    "        train = train_df[train_df['date']<=datetime.date(2017,6,15)]\n",
    "        valid = train_df[train_df['date']>datetime.date(2017,6,15)]\n",
    "        X_train = train.drop(not_used_cols, axis=1)\n",
    "        y_train = train['totals.transactionRevenue']\n",
    "        X_valid = valid.drop(not_used_cols, axis=1)\n",
    "        y_valid = valid['totals.transactionRevenue']\n",
    "        model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "                    verbose=VERBOSE_EVAL, early_stopping_rounds=STOP_ROUNDS)\n",
    "        prediction = model.predict(X_test, num_iteration=model.best_iteration_)           \n",
    "    else:\n",
    "        X = train_df.drop(not_used_cols, axis=1)\n",
    "        y = train_df['totals.transactionRevenue']\n",
    "        folds = KFold(n_splits=N_SPLITS, shuffle=True, random_state=20180917)\n",
    "        for fold_n, (train_index, test_index) in enumerate(folds.split(X)):\n",
    "            print(fold_n)\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "                    verbose=VERBOSE_EVAL, early_stopping_rounds=STOP_ROUNDS)\n",
    "            \n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            prediction += y_pred\n",
    "            print('Done with Fold:{}'.format(fold_n))\n",
    "        prediction /= N_SPLITS\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "    \n",
    "def main(sum_of_logs=False, nrows=None):\n",
    "    #Feature processing\n",
    "    ## Load data\n",
    "\n",
    "    train_data = pd.read_csv('train_modified.csv') # Important!!\n",
    "    train_df = train_data.sample(frac=0.8,random_state=200)\n",
    "    test_df = train_data.drop(train_df.index)\n",
    "\n",
    "\n",
    "    train_df = train_df\n",
    "    train_df = process_date_time(train_df)\n",
    "    test_df = test_df\n",
    "    test_df = process_date_time(test_df)\n",
    "    \n",
    "    ## Drop columns\n",
    "    train_df, test_df = drop_convert_columns(train_df, test_df)\n",
    "    \n",
    "    ## Features engineering\n",
    "    train_df = process_format(train_df)\n",
    "    train_df = process_device(train_df)\n",
    "    train_df = process_totals(train_df)\n",
    "    train_df = process_geo_network(train_df)\n",
    "    train_df = process_traffic_source(train_df)\n",
    "    \n",
    "    test_df = process_format(test_df)\n",
    "    test_df = process_device(test_df)\n",
    "    test_df = process_totals(test_df)\n",
    "    test_df = process_geo_network(test_df)\n",
    "    test_df = process_traffic_source(test_df)\n",
    "    \n",
    "    ## Categorical columns\n",
    "    not_used_cols, train_df, test_df = process_categorical_columns(train_df, test_df)\n",
    "    \n",
    "    # Model\n",
    "    prediction = model(train_df, test_df, not_used_cols)\n",
    "    \n",
    "    #Submission\n",
    "    submission(test_df, prediction)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(nrows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
